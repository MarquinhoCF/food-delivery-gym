# Food Delivery Simulator

## üìã Vis√£o Geral

O **Food Delivery Simulator** √© um simulador de entrega de comida desenvolvido utilizando a biblioteca de simula√ß√£o de eventos discretos **SimPy**. O simulador foi adaptado para funcionar como um ambiente compat√≠vel com **Gymnasium**, permitindo experimentos com **Aprendizado por Refor√ßo**. O objetivo √© testar e treinar agentes utilizando algoritmos como **PPO (Proximal Policy Optimization)** da biblioteca **Stable-Baselines3**.

![Simulador de Delivery de Comida](simulator.gif)

## üìã Requisitos

- Python 3.6 ou superior
- Gymnasium
- SimPy
- Stable-Baselines3
- RL Baselines3 Zoo
- Outras depend√™ncias listadas em `requirements.txt`

## ‚öôÔ∏è Configura√ß√£o do Ambiente

### 1Ô∏è‚É£ Criar e ativar o ambiente virtual do Python

#### No Windows:
```shell
python -m venv venv
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
python -m venv venv
source venv/bin/activate
```

### 2Ô∏è‚É£ Instalar as depend√™ncias
```shell
python -m pip install -r requirements.txt
```

### 3Ô∏è‚É£ (Apenas no Linux) Instalar `python-tk` para usar o Matplotlib
```shell
sudo apt-get install python3-tk
```

## üöÄ Uso do Simulador

### üîπ Sempre que for usar o script Python, ative o ambiente virtual:

#### No Windows:
```shell
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
source venv/bin/activate
```

### üîπ Rodar o script do teste do simulador:
```shell
python -m food_delivery_gym.examples.test
```

## üéØ Configura√ß√£o dos Cen√°rios Experimentais

O ambiente de simula√ß√£o foi formulado como um Processo de Decis√£o de Markov (MDP) voltado ao problema de entrega de √∫ltima milha. A seguir, listamos as principais constantes de configura√ß√£o para a cria√ß√£o de cen√°rios experimentais.

### üìä Par√¢metros Principais

| Vari√°vel             | Descri√ß√£o                                                                                         |
|----------------------|---------------------------------------------------------------------------------------------------|
| `NUM_DRIVERS`        | N√∫mero total de motoristas dispon√≠veis.                                                          |
| `NUM_ORDERS`         | Total de pedidos a serem gerados na simula√ß√£o.                                                   |
| `NUM_ESTABLISHMENTS` | Quantidade de restaurantes ou estabelecimentos.                                                  |
| `NUM_COSTUMERS`      | N√∫mero de clientes. Deve ser igual ao n√∫mero de pedidos.                                         |
| `GRID_MAP_SIZE`      | Tamanho do mapa da cidade (em um grid quadrado, por exemplo `50x50`).                            |
| `REWARD_OBJECTIVE`   | Define como as recompensas ser√£o calculadas. Os valores poss√≠veis v√£o de 1 a 10. Uma descri√ß√£o dos poss√≠veis `reward objectives` est√° dispon√≠vel no arquivo `food_delivery_gym/main/scenarios/reward_objectives.txt` |
| `MAX_TIME_STEP`      | Tempo m√°ximo da simula√ß√£o (em minutos).                                                          |

### üì¶ Gera√ß√£o de Pedidos

| Vari√°vel      | Descri√ß√£o                                                                                          |
|---------------|----------------------------------------------------------------------------------------------------|
| `FUNCTION`    | Fun√ß√£o lambda que define o n√∫mero de pedidos gerados por passo de tempo. Deve ser passada como uma string. |
| `TIME_SHIFT`  | Intervalo de tempo (em minutos) entre cria√ß√µes de novos pedidos.                                  |

### üöó Configura√ß√µes dos Motoristas

| Vari√°vel      | Descri√ß√£o                                                                                          |
|---------------|----------------------------------------------------------------------------------------------------|
| `VEL_DRIVERS` | Lista com [m√≠nimo, m√°ximo] de velocidade dos motoristas. Ex.: `[3, 5]`.                          |

### üè™ Configura√ß√µes dos Estabelecimentos

| Vari√°vel              | Descri√ß√£o                                                                                          |
|-----------------------|----------------------------------------------------------------------------------------------------|
| `PREPARE_TIME`        | Tempo de preparo dos pedidos: `[m√≠nimo, m√°ximo]` (em minutos).                                   |
| `OPERATING_RADIUS`    | Raio de opera√ß√£o dos estabelecimentos: `[m√≠nimo, m√°ximo]` (em unidades do grid).                 |
| `PRODUCTION_CAPACITY` | Capacidade de produ√ß√£o (n√∫mero de cozinheiros): `[m√≠nimo, m√°ximo]`.                              |

### üéõÔ∏è Aloca√ß√£o e Observa√ß√µes

| Vari√°vel                       | Descri√ß√£o                                                                                          |
|--------------------------------|----------------------------------------------------------------------------------------------------|
| `PERCENTAGE_ALLOCATION_DRIVER` | Define o percentual de preparo necess√°rio para acionar a aloca√ß√£o do motorista (ex.: `0.7`).     |
| `NORMALIZE`                    | Se `True`, normaliza as observa√ß√µes do estado para o intervalo `[0, 1]`.                         |

### üíæ Exemplo de Cen√°rio Experimental

Para configurar um cen√°rio experimental no ambiente de simula√ß√£o, √© necess√°rio criar um arquivo JSON dentro do diret√≥rio `food_delivery_gym/main/scenarios` contendo todos os par√¢metros desejados, como no exemplo abaixo:

```json
{
    "num_drivers": 10,
    "num_establishments": 10,
    "num_orders": 288,
    "num_costumers": 288,
    "grid_map_size": 50,
    "vel_drivers": [3, 5],
    "prepare_time": [20, 60],
    "operating_radius": [5, 30],
    "production_capacity": [4, 4],
    "percentage_allocation_driver": 0.7,
    "use_estimate": true,
    "desconsider_capacity": true,
    "max_time_step": 2880,
    "reward_objective": 1,
    "function_code": "lambda time: 2",
    "time_shift": 10,
    "normalize": false
}
```

Este exemplo define um cen√°rio com 10 motoristas, 288 pedidos, 10 estabelecimentos e um ambiente de 50x50 unidades. Novos pedidos s√£o criados a cada 10 minutos, 2 por vez.

### üìù Registro do Cen√°rio Experimental

Para registrar o cen√°rio ambiental criado deve ser acessado o arquivo `food_delivery_gym/__init__.py`. No arquivo o cen√°rio criado deve ser inclu√≠do seguindo o padr√£o observado e passando o nome do arquivo JSON criado anteriormente:

```python
register(
    id='food_delivery_gym/FoodDelivery-medium-obj1-v0',
    entry_point='food_delivery_gym.main.environment.food_delivery_gym_env:FoodDeliveryGymEnv',
    kwargs={
        "scenario_json_file_path": get_scenario_path("medium_obj1.json"),
    }
)
```

## ü§ñ Treinamento de Agentes de Aprendizado por Refor√ßo

Antes de iniciar o treinamento de agentes de Aprendizado por Refor√ßo (AR), √© necess√°rio **definir um cen√°rio experimental** a partir da se√ß√£o anterior.

O processo de ajuste de hiperpar√¢metros e o treinamento ser√° realizado utilizando a biblioteca [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), que fornece uma interface robusta para experimenta√ß√£o com algoritmos como PPO, DQN, A2C, entre outros.

### 1Ô∏è‚É£ Configura√ß√£o do RL Baselines3 Zoo

Siga os passos abaixo para preparar o ambiente de treinamento:

**1¬∫ Passo**: Clone o reposit√≥rio forkado do RL Baselines3 Zoo:

```bash
git clone https://github.com/MarquinhoCF/rl-baselines3-zoo.git
```

**2¬∫ Passo**: Entre na raiz do projeto `rl-baselines3-zoo`:

```bash
cd rl-baselines3-zoo
```

**3¬∫ Passo**: Crie e ative um ambiente virtual Python:

```bash
python -m venv venv
source venv/bin/activate  # No Windows, use: venv\Scripts\activate
```

**4¬∫ Passo**: Instale as depend√™ncias do Zoo:

```bash
python -m pip install -r requirements.txt
python -m pip install huggingface_hub huggingface_sb3 sb3-contrib
```

**5¬∫ Passo**: Navegue at√© o diret√≥rio do projeto `food_delivery_gym`:

```bash
cd ../food_delivery_gym/
```

**6¬∫ Passo**: Instale o pacote local:

```bash
python -m pip install .
```

**7¬∫ Passo**: Volte para o diret√≥rio do `rl-baselines3-zoo`:

```bash
cd ../rl-baselines3-zoo/
```

### 2Ô∏è‚É£ Ajuste de Hiperpar√¢metros (Opcional, mas Recomendado)

O ajuste de hiperpar√¢metros pode melhorar significativamente o desempenho do agente. Para realizar essa etapa:

**1¬∫ Passo**: Defina o n√∫mero de passos de simula√ß√£o e o n√∫mero total de tentativas no processo de tuning.

**2¬∫ Passo**: Execute o seguinte comando, substituindo os valores dos par√¢metros conforme necess√°rio. No caso do exemplo abaixo estamos fazendo o ajuste de hiperpar√¢metros do Cen√°rio M√©dio com objetivo de recompensa 1 em 1M de passos e 200 tentativas:

```bash
python train.py --algo ppo --env food_delivery_gym/FoodDelivery-medium-obj1-v0 \
--n-timesteps 1000000 --optimize-hyperparameters --max-total-trials 200 --n-jobs 2 \
--optimization-log-path logs/hyperparam_opt_ppo_food_delivery_medium_obj3/
```

### 3Ô∏è‚É£ Treinamento do Modelo

Com os hiperpar√¢metros definidos (via ajuste ou valores padr√£o), prossiga com o treinamento:

**1¬∫ Passo**: Utilize o arquivo YAML `/rl-baselines3-zoo/hyperparams/best_params_for_food_delivery_gym/ppo.yml` para definir os hiperpar√¢metros otimizados obtidos no passo anterior. Caso n√£o tenha realizado o tuning, voc√™ pode usar os par√¢metros padr√£o do `ppo`, dispon√≠veis na [documenta√ß√£o oficial do Stable-Baselines3 Zoo](https://stable-baselines3.readthedocs.io/en/master/).

```yml
food_delivery_gym/FoodDelivery-medium-obj1-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0009742009357947689
  ent_coef: 9.53697192932737e-07
  clip_range: 0.3
  n_steps: 256
  batch_size: 16
  n_epochs: 1
  gamma: 0.9912696235841435
  gae_lambda: 0.9936681407681269
  max_grad_norm: 1.92598340602166
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]), activation_fn=nn.Tanh)"
  normalize: true
```

**2¬∫ Passo**: Defina o n√∫mero total de passos de treinamento. Vamos definir 18M de passos como exemplo. Execute o treinamento:

```bash
python train.py --algo ppo --env food_delivery_gym/FoodDelivery-medium-obj1-v0 \
--conf hyperparams/best_params_for_food_delivery_gym/ppo.yml --n-timesteps 18000000
```

**3¬∫ Passo**: Visualize a curva de aprendizado:

```bash
python3 scripts/plot_train.py -a ppo -e FoodDelivery-medium-obj1-v0 -f logs/
```

## üß© Cria√ß√£o de Agentes Otimizadores com `OptimizerGym`

O pacote `food_delivery_gym` permite a cria√ß√£o de agentes otimizadores personalizados, baseados em heur√≠sticas simples ou modelos treinados com Aprendizado por Refor√ßo (AR), por meio da classe abstrata `OptimizerGym`.

Essa abordagem √© √∫til para avaliar o desempenho de algoritmos customizados no ambiente de entrega de √∫ltima milha e comparar com agentes baseados em AR.

Para desenvolver um novo agente deve herdar `OptimizerGym` e implementar o m√©todo `select_driver`.

### üîß Implementando um Otimizador Heur√≠stico

A seguir, um exemplo de implementa√ß√£o do otimizador baseado na **dist√¢ncia ao motorista mais pr√≥ximo**:

```python
from food_delivery_gym.main.optimizer.optimizer_gym.optmizer_gym import OptimizerGym
from food_delivery_gym.main.route.route import Route
from food_delivery_gym.main.driver.driver import Driver
from food_delivery_gym.main.map.map import Map
from typing import List

class NearestDriverOptimizerGym(OptimizerGym):
    def get_title(self):
        return "Otimizador do Motorista Mais Pr√≥ximo"

    def compare_distance(self, map: Map, driver: Driver, route: Route):
        return map.distance(driver.get_last_coordinate_from_routes_list(), route.route_segments[0].coordinate)

    def select_driver(self, obs: dict, drivers: List[Driver], route: Route):
        nearest_driver = min(drivers, key=lambda driver: self.compare_distance(self.gym_env.simpy_env.map, driver, route))
        return drivers.index(nearest_driver)
```

### ü§ñ Implementando um Otimizador com Modelo de AR (PPO)

Se voc√™ j√° treinou um modelo com o RL Baselines3 Zoo (como mostrado na se√ß√£o anterior), pode integr√°-lo diretamente:

```python
from stable_baselines3 import PPO
from food_delivery_gym.main.optimizer.optimizer_gym.optmizer_gym import OptimizerGym
from food_delivery_gym.main.route.route import Route
from food_delivery_gym.main.driver.driver import Driver
from typing import List, Union
import numpy as np

class RLModelOptimizerGym(OptimizerGym):
    def __init__(self, environment, model: PPO):
        super().__init__(environment)
        self.model = model

    def get_title(self):
        return "Otimizador por Aprendizado por Refor√ßo"

    def select_driver(self, obs: dict, drivers: List[Driver], route: Route):
        action, _ = self.model.predict(obs, deterministic=True)
        return int(action) if isinstance(action, (int, np.integer)) else action.item()
```

### ‚ñ∂Ô∏è Executando Simula√ß√µes com o Otimizador

Com seu otimizador implementado, basta instanci√°-lo e chamar:

```python
optimizer = NearestDriverOptimizerGym(env)
# ou
optimizer = RLModelOptimizerGym(env, trained_model)

optimizer.run_simulations(num_runs=10, dir_path="./resultados/", seed=42)
```

Isso executar√° m√∫ltiplas simula√ß√µes, coletar√° estat√≠sticas (recompensa, tempo de entrega, dist√¢ncia percorrida, etc.) e salvar√° os resultados em um arquivo `.txt` e `.npz`.

## üß™ Execu√ß√£o em Lote de Otimizadores e Gera√ß√£o de Tabelas

Para facilitar a execu√ß√£o massiva de simula√ß√µes e a gera√ß√£o de tabelas com os resultados dos agentes otimizadores (heur√≠sticos e baseados em AR), o projeto fornece dois scripts utilit√°rios:

### üöÄ Script `run_optimizer`: Execu√ß√£o de M√∫ltiplos Otimizadores

Esse script automatiza a execu√ß√£o de diferentes agentes otimizadores em todos os **cen√°rios experimentais** (`initial`, `medium`, `complex`) e para todos os **10 objetivos de recompensa**.

#### ‚úÖ O que ele faz:

* Executa os seguintes agentes:
  * `RandomDriverOptimizerGym`
  * `FirstDriverOptimizerGym`
  * `NearestDriverOptimizerGym`
  * `LowestCostDriverOptimizerGym`
  * `RLModelOptimizerGym` (com diferentes checkpoints de modelos PPO treinados)
* Gera arquivos `.txt` com os resultados das execu√ß√µes
* Gera arquivos `.npz` contendo as m√©tricas agregadas para an√°lise

#### üì¶ Como usar:

Execute o comando:

```bash
python -m scripts.run_optimizer
```

> **Observa√ß√£o:** o script procura modelos PPO previamente treinados nos diret√≥rios definidos em `MODEL_BASE_DIR`. Certifique-se de que os modelos `.zip` e arquivos `vecnormalize.pkl` est√£o no local correto para que o RL funcione.

### üìä Script `generate_table`: Gera√ß√£o de Planilhas Excel com M√©tricas

Esse script consolida os resultados gerados pelo `run_optimizer` e preenche automaticamente um modelo Excel (`template_objective_table.xlsx`) com os dados das m√©tricas estat√≠sticas:

* **Recompensas**
* **Tempo efetivo de entrega**
* **Dist√¢ncia total percorrida**

#### ‚úÖ O que ele faz:

* Para cada heur√≠stica e modelo PPO, em cada cen√°rio e objetivo, extrai as m√©tricas do arquivo `metrics_data.npz`
* Preenche as abas do Excel com m√©dia, desvio padr√£o, mediana e moda
* Gera um novo arquivo: `objective_table.xlsx`

#### üì¶ Como usar:

1. Garanta que o script `run_optimizer` j√° foi executado e os arquivos `metrics_data.npz` foram gerados.
2. Certifique-se de ter o template em: `./templates/template_objective_table.xlsx`
3. Execute o comando:

```bash
python -m scripts.generate_table
```

> O Excel final ser√° salvo com o nome `objective_table.xlsx` no diret√≥rio atual.
# Food Delivery Simulator

## ðŸ“‹ VisÃ£o Geral

O **Food Delivery Simulator** Ã© um simulador de entrega de comida desenvolvido utilizando a biblioteca de simulaÃ§Ã£o de eventos discretos **SimPy**. O simulador foi adaptado para funcionar como um ambiente compatÃ­vel com **Gymnasium**, permitindo experimentos com **Aprendizado por ReforÃ§o**. O objetivo Ã© testar e treinar agentes utilizando algoritmos como **PPO (Proximal Policy Optimization)** da biblioteca **Stable-Baselines3**.

![Simulador de Delivery de Comida](simulator.gif)

## ðŸ“‹ Requisitos

- Python 3.6 ou superior
- Gymnasium
- SimPy
- Stable-Baselines3
- RL Baselines3 Zoo
- Outras dependÃªncias listadas em `requirements.txt`

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

### 1ï¸âƒ£ Criar e ativar o ambiente virtual do Python

#### No Windows:
```shell
python -m venv venv
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
python3 -m venv venv
source venv/bin/activate
```

### 2ï¸âƒ£ Instalar as dependÃªncias
```shell
python -m pip install -r requirements.txt
```

### 3ï¸âƒ£ (Apenas no Linux) Instalar `python-tk` para usar o Matplotlib
```shell
sudo apt-get install python3-tk
```

## ðŸš€ Uso do Simulador

### ðŸ”¹ Sempre que for usar o script Python, ative o ambiente virtual:

#### No Windows:
```shell
Set-ExecutionPolicy Unrestricted -Scope Process
.\venv\Scripts\activate
```

#### No Linux/Mac:
```shell
source venv/bin/activate
```

### ðŸ”¹ Rodar o script do teste do simulador:

#### 1. **Modo AutomÃ¡tico**
```shell
python -m scripts.test_runner --mode auto --scenario medium_obj1.json --render
```
- Executa automaticamente com aÃ§Ãµes aleatÃ³rias
- Ãštil para testes rÃ¡pidos

#### 2. **Modo Interativo** (Recomendado para desenvolvimento)
```shell
python -m scripts.test_runner --mode interactive --scenario medium_obj1.json --render
```
- Executa passo-a-passo esperando sua entrada
- **Comandos disponÃ­veis**:
  - `Enter`: aÃ§Ã£o aleatÃ³ria
  - `run`: executa automaticamente atÃ© o fim
  - `quit`: sai do programa
  - NÃºmeros/aÃ§Ãµes: entrada manual personalizada

#### 3. **Modo com Agente PPO** (Requer modelo treinado)
```shell
python -m scripts.test_runner --mode agent --scenario medium_obj1.json --model-path models/ppo_food_delivery --render
```

### âš™ï¸ OpÃ§Ãµes de ConfiguraÃ§Ã£o

| OpÃ§Ã£o | DescriÃ§Ã£o | Exemplo |
|-------|-----------|---------|
| `--mode` | Modo de execuÃ§Ã£o: `interactive`, `auto`, `agent` | `--mode interactive` |
| `--scenario` | Arquivo de cenÃ¡rio JSON | `--scenario medium_obj1.json` |
| `--render` | Ativa visualizaÃ§Ã£o grÃ¡fica | `--render` |
| `--seed` | Seed para reproducibilidade | `--seed 42` |
| `--max-steps` | Limite mÃ¡ximo de passos | `--max-steps 1000` |
| `--save-log` | Salva output em arquivo log.txt | `--save-log` |
| `--model-path` | Caminho para modelo PPO (modo agent) | `--model-path models/ppo_model` |

#### Exemplos:

```shell
# Teste rÃ¡pido com cenÃ¡rio padrÃ£o
python -m scripts.test_runner --mode auto --max-steps 100

# Debug interativo com renderizaÃ§Ã£o
python -m scripts.test_runner --mode interactive --render --seed 123

# Teste de performance com log
python -m scripts.test_runner --mode auto --max-steps 5000 --save-log

# Testar cenÃ¡rio especÃ­fico
python -m scripts.test_runner --scenario meu_cenario.json --mode interactive --render
```

## ðŸŽ¯ ConfiguraÃ§Ã£o dos CenÃ¡rios Experimentais

O ambiente de simulaÃ§Ã£o foi formulado como um Processo de DecisÃ£o de Markov (MDP) voltado ao problema de entrega de Ãºltima milha. A seguir, listamos as principais constantes de configuraÃ§Ã£o para a criaÃ§Ã£o de cenÃ¡rios experimentais.

### ðŸ“Š ParÃ¢metros Principais

| VariÃ¡vel             | DescriÃ§Ã£o                                                                                         |
|----------------------|---------------------------------------------------------------------------------------------------|
| `NUM_DRIVERS`        | NÃºmero total de motoristas disponÃ­veis.                                                          |
| `NUM_ORDERS`         | Total de pedidos a serem gerados na simulaÃ§Ã£o.                                                   |
| `NUM_ESTABLISHMENTS` | Quantidade de restaurantes ou estabelecimentos.                                                  |
| `NUM_COSTUMERS`      | NÃºmero de clientes. Deve ser igual ao nÃºmero de pedidos.                                         |
| `GRID_MAP_SIZE`      | Tamanho do mapa da cidade (em um grid quadrado, por exemplo `50x50`).                            |
| `REWARD_OBJECTIVE`   | Define como as recompensas serÃ£o calculadas. Os valores possÃ­veis vÃ£o de 1 a 10. Uma descriÃ§Ã£o dos possÃ­veis `reward objectives` estÃ¡ disponÃ­vel no arquivo `food_delivery_gym/main/scenarios/reward_objectives.txt` |
| `MAX_TIME_STEP`      | Tempo mÃ¡ximo da simulaÃ§Ã£o (em minutos).                                                          |

### ðŸ“¦ GeraÃ§Ã£o de Pedidos

| VariÃ¡vel      | DescriÃ§Ã£o                                                                                          |
|---------------|----------------------------------------------------------------------------------------------------|
| `FUNCTION`    | FunÃ§Ã£o lambda que define o nÃºmero de pedidos gerados por passo de tempo. Deve ser passada como uma string. |
| `TIME_SHIFT`  | Intervalo de tempo (em minutos) entre criaÃ§Ãµes de novos pedidos.                                  |

### ðŸš— ConfiguraÃ§Ãµes dos Motoristas

| VariÃ¡vel      | DescriÃ§Ã£o                                                                                          |
|---------------|----------------------------------------------------------------------------------------------------|
| `VEL_DRIVERS` | Lista com [mÃ­nimo, mÃ¡ximo] de velocidade dos motoristas. Ex.: `[3, 5]`.                          |

### ðŸª ConfiguraÃ§Ãµes dos Estabelecimentos

| VariÃ¡vel              | DescriÃ§Ã£o                                                                                          |
|-----------------------|----------------------------------------------------------------------------------------------------|
| `PREPARE_TIME`        | Tempo de preparo dos pedidos: `[mÃ­nimo, mÃ¡ximo]` (em minutos).                                   |
| `OPERATING_RADIUS`    | Raio de operaÃ§Ã£o dos estabelecimentos: `[mÃ­nimo, mÃ¡ximo]` (em unidades do grid).                 |
| `PRODUCTION_CAPACITY` | Capacidade de produÃ§Ã£o (nÃºmero de cozinheiros): `[mÃ­nimo, mÃ¡ximo]`.                              |

### ðŸŽ›ï¸ AlocaÃ§Ã£o e ObservaÃ§Ãµes

| VariÃ¡vel                       | DescriÃ§Ã£o                                                                                          |
|--------------------------------|----------------------------------------------------------------------------------------------------|
| `PERCENTAGE_ALLOCATION_DRIVER` | Define o percentual de preparo necessÃ¡rio para acionar a alocaÃ§Ã£o do motorista (ex.: `0.7`).     |
| `NORMALIZE`                    | Se `True`, normaliza as observaÃ§Ãµes do estado para o intervalo `[0, 1]`.                         |

### ðŸ’¾ Exemplo de CenÃ¡rio Experimental

Para configurar um cenÃ¡rio experimental no ambiente de simulaÃ§Ã£o, Ã© necessÃ¡rio criar um arquivo JSON dentro do diretÃ³rio `food_delivery_gym/main/scenarios` contendo todos os parÃ¢metros desejados, como no exemplo abaixo:

```json
{
    "num_drivers": 10,
    "num_establishments": 10,
    "num_orders": 288,
    "num_costumers": 288,
    "grid_map_size": 50,
    "vel_drivers": [3, 5],
    "prepare_time": [20, 60],
    "operating_radius": [5, 30],
    "production_capacity": [4, 4],
    "percentage_allocation_driver": 0.7,
    "use_estimate": true,
    "desconsider_capacity": true,
    "max_time_step": 2880,
    "reward_objective": 1,
    "function_code": "lambda time: 2",
    "time_shift": 10,
    "normalize": false
}
```

Este exemplo define um cenÃ¡rio com 10 motoristas, 288 pedidos, 10 estabelecimentos e um ambiente de 50x50 unidades. Novos pedidos sÃ£o criados a cada 10 minutos, 2 por vez.

### ðŸ“ Registro do CenÃ¡rio Experimental

Para registrar o cenÃ¡rio ambiental criado deve ser acessado o arquivo `food_delivery_gym/__init__.py`. No arquivo o cenÃ¡rio criado deve ser incluÃ­do seguindo o padrÃ£o observado e passando o nome do arquivo JSON criado anteriormente:

```python
register(
    id='food_delivery_gym/FoodDelivery-medium-obj1-v0',
    entry_point='food_delivery_gym.main.environment.food_delivery_gym_env:FoodDeliveryGymEnv',
    kwargs={
        "scenario_json_file_path": get_scenario_path("medium_obj1.json"),
    }
)
```

## ðŸ¤– Treinamento de Agentes de Aprendizado por ReforÃ§o

Antes de iniciar o treinamento de agentes de Aprendizado por ReforÃ§o (AR), Ã© necessÃ¡rio **definir um cenÃ¡rio experimental** a partir da seÃ§Ã£o anterior.

O processo de ajuste de hiperparÃ¢metros e o treinamento serÃ¡ realizado utilizando a biblioteca [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), que fornece uma interface robusta para experimentaÃ§Ã£o com algoritmos como PPO, DQN, A2C, entre outros.

### 1ï¸âƒ£ ConfiguraÃ§Ã£o do RL Baselines3 Zoo

Siga os passos abaixo para preparar o ambiente de treinamento:

**1Âº Passo**: Clone o repositÃ³rio forkado do RL Baselines3 Zoo:

```bash
git clone https://github.com/MarquinhoCF/rl-baselines3-zoo.git
```

**2Âº Passo**: Entre na raiz do projeto `rl-baselines3-zoo`:

```bash
cd rl-baselines3-zoo
```

**3Âº Passo**: Crie e ative um ambiente virtual Python:

```bash
python3 -m venv venv
source venv/bin/activate  # No Windows, use: venv\Scripts\activate
```

**4Âº Passo**: Instale as dependÃªncias do Zoo:

```bash
python -m pip install -r requirements.txt
python -m pip install huggingface_hub huggingface_sb3 sb3-contrib optuna
```

**5Âº Passo**: Navegue atÃ© o diretÃ³rio do projeto `food-delivery-gym`:

```bash
cd ../food-delivery-gym/
```

**6Âº Passo**: Instale o pacote local:

```bash
python -m pip install .
```

**7Âº Passo**: Volte para o diretÃ³rio do `rl-baselines3-zoo`:

```bash
cd ../rl-baselines3-zoo/
```

### 2ï¸âƒ£ Ajuste de HiperparÃ¢metros (Opcional, mas Recomendado)

O ajuste de hiperparÃ¢metros pode melhorar significativamente o desempenho do agente. Para realizar essa etapa:

**1Âº Passo**: Defina o nÃºmero de passos de simulaÃ§Ã£o e o nÃºmero total de tentativas no processo de tuning.

**2Âº Passo**: Execute o seguinte comando, substituindo os valores dos parÃ¢metros conforme necessÃ¡rio. No caso do exemplo abaixo estamos fazendo o ajuste de hiperparÃ¢metros do CenÃ¡rio MÃ©dio com objetivo de recompensa 1 em 1M de passos e 200 tentativas:

```bash
python train.py --algo ppo --env food_delivery_gym/FoodDelivery-medium-obj1-v0 \
--n-timesteps 1000000 --optimize-hyperparameters --max-total-trials 200 --n-jobs 2 \
--optimization-log-path logs/hyperparam_opt_ppo_food_delivery_medium_obj3/
```

### 3ï¸âƒ£ Treinamento do Modelo

Com os hiperparÃ¢metros definidos (via ajuste ou valores padrÃ£o), prossiga com o treinamento:

**1Âº Passo**: Utilize o arquivo YAML `/rl-baselines3-zoo/hyperparams/best_params_for_food_delivery_gym/ppo.yml` para definir os hiperparÃ¢metros otimizados obtidos no passo anterior. Caso nÃ£o tenha realizado o tuning, vocÃª pode usar os parÃ¢metros padrÃ£o do `ppo`, disponÃ­veis na [documentaÃ§Ã£o oficial do Stable-Baselines3 Zoo](https://stable-baselines3.readthedocs.io/en/master/).

```yml
food_delivery_gym/FoodDelivery-medium-obj1-v0:
  n_timesteps: 18000000
  policy: 'MultiInputPolicy'
  n_envs: 4 
  learning_rate: 0.0009742009357947689
  ent_coef: 9.53697192932737e-07
  clip_range: 0.3
  n_steps: 256
  batch_size: 16
  n_epochs: 1
  gamma: 0.9912696235841435
  gae_lambda: 0.9936681407681269
  max_grad_norm: 1.92598340602166
  policy_kwargs: "dict(net_arch=dict(pi=[64], vf=[64]), activation_fn=nn.Tanh)"
  normalize: true
```

**2Âº Passo**: Defina o nÃºmero total de passos de treinamento. Vamos definir 18M de passos como exemplo. Execute o treinamento:

```bash
python train.py --algo ppo --env food_delivery_gym/FoodDelivery-medium-obj1-v0 \
--conf hyperparams/best_params_for_food_delivery_gym/ppo.yml --n-timesteps 18000000
```

**3Âº Passo**: Visualize a curva de aprendizado:

```bash
python scripts/plot_train.py -a ppo -e FoodDelivery-medium-obj1-v0 -f logs/
```

## ðŸ§© CriaÃ§Ã£o de Agentes Otimizadores com `OptimizerGym`

O pacote `food_delivery_gym` permite a criaÃ§Ã£o de agentes otimizadores personalizados, baseados em heurÃ­sticas simples ou modelos treinados com Aprendizado por ReforÃ§o (AR), por meio da classe abstrata `OptimizerGym`.

Essa abordagem Ã© Ãºtil para avaliar o desempenho de algoritmos customizados no ambiente de entrega de Ãºltima milha e comparar com agentes baseados em AR.

Para desenvolver um novo agente deve herdar `OptimizerGym` e implementar o mÃ©todo `select_driver`.

### ðŸ”§ Implementando um Otimizador HeurÃ­stico

A seguir, um exemplo de implementaÃ§Ã£o do otimizador baseado na **distÃ¢ncia ao motorista mais prÃ³ximo**:

```python
from food_delivery_gym.main.optimizer.optimizer_gym.optmizer_gym import OptimizerGym
from food_delivery_gym.main.route.route import Route
from food_delivery_gym.main.driver.driver import Driver
from food_delivery_gym.main.map.map import Map
from typing import List

class NearestDriverOptimizerGym(OptimizerGym):
    def get_title(self):
        return "Otimizador do Motorista Mais PrÃ³ximo"

    def compare_distance(self, map: Map, driver: Driver, route: Route):
        return map.distance(driver.get_last_valid_coordinate(), route.route_segments[0].coordinate)

    def select_driver(self, obs: dict, drivers: List[Driver], route: Route):
        nearest_driver = min(drivers, key=lambda driver: self.compare_distance(self.gym_env.simpy_env.map, driver, route))
        return drivers.index(nearest_driver)
```

### ðŸ¤– Implementando um Otimizador com Modelo de AR (PPO)

Se vocÃª jÃ¡ treinou um modelo com o RL Baselines3 Zoo (como mostrado na seÃ§Ã£o anterior), pode integrÃ¡-lo diretamente:

```python
from stable_baselines3 import PPO
from food_delivery_gym.main.optimizer.optimizer_gym.optmizer_gym import OptimizerGym
from food_delivery_gym.main.route.route import Route
from food_delivery_gym.main.driver.driver import Driver
from typing import List, Union
import numpy as np

class RLModelOptimizerGym(OptimizerGym):
    def __init__(self, environment, model: PPO):
        super().__init__(environment)
        self.model = model

    def get_title(self):
        return "Otimizador por Aprendizado por ReforÃ§o"

    def select_driver(self, obs: dict, drivers: List[Driver], route: Route):
        action, _ = self.model.predict(obs, deterministic=True)
        return int(action) if isinstance(action, (int, np.integer)) else action.item()
```

### â–¶ï¸ Executando SimulaÃ§Ãµes com o Otimizador

Com seu otimizador implementado, basta instanciÃ¡-lo e chamar:

```python
optimizer = NearestDriverOptimizerGym(env)
# ou
optimizer = RLModelOptimizerGym(env, trained_model)

optimizer.run_simulations(num_runs=10, dir_path="./resultados/", seed=42)
```

Isso executarÃ¡ mÃºltiplas simulaÃ§Ãµes, coletarÃ¡ estatÃ­sticas (recompensa, tempo de entrega, distÃ¢ncia percorrida, etc.) e salvarÃ¡ os resultados em um arquivo `.txt` e `.npz`.

## ðŸ§ª ExecuÃ§Ã£o em Lote de Otimizadores e GeraÃ§Ã£o de Tabelas

Para facilitar a execuÃ§Ã£o massiva de simulaÃ§Ãµes e a geraÃ§Ã£o de tabelas com os resultados dos agentes otimizadores (heurÃ­sticos e baseados em AR), o projeto fornece dois scripts utilitÃ¡rios:

### ðŸš€ Script `run_optimizer`: ExecuÃ§Ã£o de MÃºltiplos Otimizadores

Esse script automatiza a execuÃ§Ã£o de diferentes agentes otimizadores em todos os **cenÃ¡rios experimentais** (`initial`, `medium`, `complex`) e para todos os **10 objetivos de recompensa**.

#### âœ… O que ele faz:

* Executa os seguintes agentes:
  * `RandomDriverOptimizerGym`
  * `FirstDriverOptimizerGym`
  * `NearestDriverOptimizerGym`
  * `LowestCostDriverOptimizerGym`
  * `RLModelOptimizerGym` (com diferentes checkpoints de modelos PPO treinados)
* Gera arquivos `.txt` com os resultados das execuÃ§Ãµes
* Gera arquivos `.npz` contendo as mÃ©tricas agregadas para anÃ¡lise

#### ðŸ“¦ Como usar:

Execute o comando:

```bash
python -m scripts.run_optimizer
```

> **ObservaÃ§Ã£o:** o script procura modelos PPO previamente treinados nos diretÃ³rios definidos em `MODEL_BASE_DIR`. Certifique-se de que os modelos `.zip` e arquivos `vecnormalize.pkl` estÃ£o no local correto para que o RL funcione.

### ðŸ“Š Script `generate_table`: GeraÃ§Ã£o de Planilhas Excel com MÃ©tricas

Esse script consolida os resultados gerados pelo `run_optimizer` e preenche automaticamente um modelo Excel (`template_objective_table.xlsx`) com os dados das mÃ©tricas estatÃ­sticas:

* **Recompensas**
* **Tempo efetivo de entrega**
* **DistÃ¢ncia total percorrida**

#### âœ… O que ele faz:

* Para cada heurÃ­stica e modelo PPO, em cada cenÃ¡rio e objetivo, extrai as mÃ©tricas do arquivo `metrics_data.npz`
* Preenche as abas do Excel com mÃ©dia, desvio padrÃ£o, mediana e moda
* Gera um novo arquivo: `objective_table.xlsx`

#### ðŸ“¦ Como usar:

1. Garanta que o script `run_optimizer` jÃ¡ foi executado e os arquivos `metrics_data.npz` foram gerados.
2. Certifique-se de ter o template em: `./templates/template_objective_table.xlsx`
3. Execute o comando:

```bash
python -m scripts.generate_table
```

> O Excel final serÃ¡ salvo com o nome `objective_table.xlsx` no diretÃ³rio atual.

---

## ðŸ“‚ Acesso aos Dados Experimentais

Os resultados completos dos experimentos realizados com o simulador, incluindo logs, mÃ©tricas agregadas, modelos treinados, arquivos de normalizaÃ§Ã£o e tabelas comparativas, estÃ£o disponÃ­veis na nuvem.

A estrutura de diretÃ³rios de saÃ­da segue o seguinte padrÃ£o:

```
data/
â”œâ”€â”€ ppo_training/
â”‚   â””â”€â”€ otimizacao_1M_steps_200_trials
â”‚        â”œâ”€â”€ otimizaÃ§Ã£o
â”‚        â”‚    â”œâ”€â”€ logs
â”‚        â”‚    â”‚    â””â”€â”€ ... (modelos treinados e logs)
â”‚        â”‚    â””â”€â”€ ppo_best_hyperparameters_food_delivery_gym.yml  # Resultados dos ajustes de hiperparÃ¢metros
â”‚        â””â”€â”€ treinamento
â”‚             â””â”€â”€ ... (modelos treinados e arquivos de normalizaÃ§Ã£o)
â””â”€â”€ runs/
    â””â”€â”€ execucoes/
        â”œâ”€â”€ obj_<N>/               # Objetivos 1 a 10
        â”‚    â””â”€â”€ <cenario>_scenario/
        â”‚        â””â”€â”€ <heuristica>/
        â”‚            â”œâ”€â”€ results.txt
        â”‚            â”œâ”€â”€ mean_results_(valor).png
        â”‚            â””â”€â”€ figs/
        â””â”€â”€ objective_table.xlsx   # Tabela consolidada com resultados

notebooks/
â”œâ”€â”€ reward_objective_graphs.ipynb (Notebook com anÃ¡lises preliminares das funÃ§Ãµes de recompensa)
â”‚
â””â”€â”€ data_notebooks
     â”œâ”€â”€ ppo_training/
     â”‚   â””â”€â”€ ... (modelos treinados, logs e resultados)
     â”‚  
     â”‚       
     â”‚        
     â”‚             
     â””â”€â”€ runs/
         â””â”€â”€ obj_<N>/               # Objetivos 1 a 10
              â””â”€â”€ <cenario>_scenario/
                  â””â”€â”€ <heuristica>/
                      â”œâ”€â”€ results.txt
                      â”œâ”€â”€ metrics_data.npz
                      â”œâ”€â”€ mean_results_(valor).png
                      â””â”€â”€ figs/
```

> ðŸ“‚ **Download dos dados completos (Google Drive):**
> [Clique aqui para acessar](https://drive.google.com/drive/folders/1YzpAzy5L5YcqjMntWio_5JnyfXeccu-S?usp=sharing)

Caso deseje rodar os scripts localmente com todos os dados originais, baixe e extraia o conteÃºdo do diretÃ³rio `data/`, de forma a garantir que a estrutura, descrita na seÃ§Ã£o anterior, se mantenha.
